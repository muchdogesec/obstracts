# Environment file info

If you're running in production, you should set these securely.

However, if you just want to experiment, set the following values

## Django Settings

These are all Django settings, defined in `obstracts/settings.py`

* `DJANGO_SECRET`: `insecure_django_secret`
* `DJANGO_DEBUG`: `True`
* `DJANGO_ALLOWED_HOSTS`: BLANK
* `DJANGO_CORS_ALLOW_ALL_ORIGINS`: `True`
* `DJANGO_CORS_ALLOWED_ORIGINS`: LEAVE EMPTY

## Postgres Settings

These are all Django settings, defined in `obstracts/settings.py`

* `POSTGRES_HOST`: `pgdb`
* `POSTGRES_PORT`: BLANK
* `POSTGRES_DB`: `postgres`
* `POSTGRES_USER`: `postgres`
* `POSTGRES_PASSWORD`: `postgres`

## Celery settings

* `CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP`: `1`
* `CELERY_BROKER_URL`: `redis://host.docker.internal:6379/3`
* `result_backend`: `redis://host.docker.internal:6379/1`

## Obstracts API settings

These define how the API behaves.

* `MAX_PAGE_SIZE`: `50`
	* This is the maximum number of results the API will ever return before pagination
* `DEFAULT_PAGE_SIZE`: `50`
	* The default page size of result returned by the API

## ArangoDB settings

Note, this code will not install an ArangoDB instance.

If you're new to ArangoDB, [you can install the community edition quickly by following the instructions here](https://arangodb.com/community-server/).

The script will automatically create a database called `obstracts_database` when the container is spun up (if it does not exist).

For each blog added, two new collections will be created in the format

`<FEED_NAME>_<FEED_ID>-<COLLECTION_TYPE>_collection`

e.g.

* `graham_cluley_9288374-0298740-94875-vertex_collection`
* `graham_cluley_9288374-0298740-94875-edge_collection`


* `ARANGODB_HOST_URL`: `'http://host.docker.internal:8529'`
	* If you are running ArangoDB locally, be sure to set `ARANGODB_HOST_URL='http://host.docker.internal:8529'` in the `.env` file otherwise you will run into networking errors.
* `ARANGODB_USERNAME`: `root`
	* Change this if neeed
* `ARANGODB_PASSWORD`: USE PASSWORD OF ARANGODB_USERNAME

## AI Settings

* `INPUT_TOKEN_LIMIT`: `15000`
	* (REQUIRED IF USING AI MODES) Ensure the input/output token count meets requirements and is supported by the model selected. Will not allow files with more than tokens specified to be processed
* `TEMPERATURE`: `0.0` 
	* The temperature value ranges from 0 to 2, with lower values indicating greater determinism and higher values indicating more randomness in responses.

**A small note on selecting a provider**

Below are the models you can use.

We strongly recommend using OpenAI because of it's use of structured outputs. Whilst the other models should work, they can often ignore prompts for our expected response structured leading to failures.

* `OPENAI_API_KEY`: YOUR_API_KEY
	* (REQUIRED IF USING OPENAI MODELS DIRECTLY IN AI MODES) get it from: https://platform.openai.com/api-keys
* `OPENROUTER_API_KEY`=
	* (REQUIRED IF USING MODELS PROVIDED BY OPENROUTER IN AI MODES) get it from: https://openrouter.ai/settings/keys
* `DEEPSEEK_API_KEY`=
	* (REQUIRED IF USING DEEPSEEK MODELS DIRECTLY IN AI MODES) get it from: https://platform.deepseek.com/api-key
* `ANTHROPIC_API_KEY`: YOUR_API_KEY
	* (REQUIRED IF USING ANTHROPIC MODELS DIRECTLY IN AI MODES) get it from" https://console.anthropic.com/settings/keys
* `GOOGLE_API_KEY`:
	* (REQUIRED IF USING GOOGLE GEMINI MODELS DIRECTLY IN AI MODES) get it from the Google Cloud Platform (making sure the Gemini API is enabled for the project)

## BIN List

* `BIN_LIST_API_KEY`: BLANK
	*  for enriching credit card extractions needed for extracting credit card information. You get an API key here https://rapidapi.com/trade-expanding-llc-trade-expanding-llc-default/api/bin-ip-checker

## CTIBUTLER

Obstracts requires [ctibutler](https://github.com/muchdogesec/ctibutler) to lookup ATT&CK, CAPEC, CWE, ATLAS, and locations

* `CTIBUTLER_BASE_URL`: `'http://api.ctibutler.com'` (recommended)
	* If you are running CTI Butler locally, be sure to set `'http://host.docker.internal:8006/api/'` in the `.env` file otherwise you will run into networking errors.
* `CTIBUTLER_API_KEY`:
	* If using `'http://api.ctibutler.com'`, [get your API key here](http://app.ctibutler.com). Can be left blank if running locally.

## VULMATCH FOR CVE AND CPE LOOKUPS

Obstracts requires [vulmatch](https://github.com/muchdogesec/vulmatch) to lookup CVEs and CPEs

* `VULMATCH_BASE_URL`: `'http://api.vulmatch.com'` (recommended)
	* If you are running Vulmatch locally, be sure to set `'http://host.docker.internal:8005/api/'` in the `.env` file otherwise you will run into networking errors.
* `VULMATCH_API_KEY`:
	* If using `'http://api.vulmatch.com'`, [get your API key here](http://app.vulmatch.com). Can be left blank if running locally.

## file2txt settings

* `GOOGLE_VISION_API_KEY`: YOUR_API_KEY
	* Instructions here: https://github.com/muchdogesec/file2txt?tab=readme-ov-file#optional-add-googles-cloud-vision-api-key

## R2 storage configuration

You can choose to store static assets on Cloudflare on R2. Default is local.

* `USE_S3_STORAGE`: `0`
	* Set to `1` to enable
* `R2_ENDPOINT_URL`: BLANK
	* Will be something like `https://ID.r2.cloudflarestorage.com`
* `R2_BUCKET_NAME`: BLANK
	* The bucket name you want to use.
* `R2_ACCESS_KEY`: BLANK
	* generated when creating an R2 API token. Make sure has read+write to R2_`BUCKET_NAME` specified
* `R2_SECRET_KEY`: BLANK
	* generated when creating an R2 API token
* `R2_CUSTOM_DOMAIN`: BLANK
	* this value is optional when using R2, but if you don't set your bucket to public, your images will hit 403s as they will hit the raw endpoint (e.g. https://ID.r2.cloudflarestorage.com/BUCKET/IMAGE/PATH.jpg) which will be inaccessible. The easiest way to do this is to enable R2.dev subdomain for the bucket. Looks like `pub-ID.r2.dev` . Do not include the `https://` part

## DOGESEC COMMONS

* `SRO_OBJECTS_ONLY_LATEST`: `False`
	* Due to the way stix2arango works, SCOs with same ID (and also relationships connecting them to other object) will be aged out . This doesn't work in Stixify because we create the same SCO for many reports, but only one is ever `_is_latest=true` due to S2A versioning logic. This value should always be set to `false` to ensure all relationships to SCOs are shown via the API. [You can read more about this in this issue](https://github.com/muchdogesec/dogesec_commons/issues/47)

## history4feed

Obstracts runs [history4feed](https://github.com/muchdogesec/history4feed) as a library and will configure and run it for you automatically on run.

* `HISTORY4FEED_EARLIEST_SEARCH_DATE`: `2020-01-01T00:00:00Z`
	* determines how far history4feed will backfill posts for newly added feeds. e.g. `EARLIEST_SEARCH_DATE=2020-01-01T00:00:00Z` will import all posts with a publish date >= `2020-01-01T00:00:00Z`
* `SCRAPFILE_APIKEY`: YOUR_API_KEY
	* We strongly recommend using the [ScrapFly](https://scrapfly.io/) proxy service with history4feed. Though we have no affiliation with them, it is the best proxy service we've tested and thus built in support for it to history4feed.

If you're not using a Proxy it is very likely you'll run into rate limits on the WayBack Machine and the blogs you're requesting the full text from. You should therefore consider the following options

* `HISTORY4FEED_WAYBACK_SLEEP_SECONDS`: `20`
	* This is useful when a large amount of posts are returned. This sets the time between each request to get the full text of the article to reduce servers blocking robotic requests.
* `HISTORY4FEED_REQUEST_RETRY_COUNT`: `3`
	* This is useful when a large amount of posts are returned. This sets the number of retries when a non-200 response is returned.